{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanbox - Py36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running system under path: C:\\Users\\patblo\\anaconda3\n",
      "\n",
      "System version: 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "Pandas version: 0.24.2\n",
      "Numpy version: 1.17.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import calendar \n",
    "import pydoc\n",
    "import types\n",
    "import xlrd\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "import sys as sys\n",
    "import os as os\n",
    "import operator as operator\n",
    "from collections import Counter\n",
    "\n",
    "print (\"Running system under path: {}\\n\".format(sys.prefix))\n",
    "\n",
    "print (\"System version: {}\".format(sys.version))\n",
    "print (\"Pandas version: {}\".format(pd.__version__))\n",
    "print (\"Numpy version: {}\".format(np.__version__))\n",
    "print (\"\\n\")\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Spear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patblo\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Users\\patblo\\anaconda3\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, learning_curve, ShuffleSplit, RandomizedSearchCV, train_test_split, cross_val_score, validation_curve, KFold\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.feature_selection import RFE,SelectKBest\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import _name_estimators\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.externals import six\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.base import clone\n",
    "from sklearn.base import TransformerMixin\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Widen notebook\n",
    "from IPython.core.display import HTML, display\n",
    "HTML(\"<style>.container { width:85% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import relativedelta as rtd\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "#--------------------------------- \n",
    "# TIME DIMENSIONS\n",
    "#---------------------------------\n",
    "def f_this_ym():\n",
    "    return dt.date.today().strftime('%Y%m')\n",
    "\n",
    "def f_last_ym():\n",
    "    return (dt.date.today() - rtd.relativedelta(months=+1)).strftime(\"%Y%m\")\n",
    "\n",
    "def f_last_ymd_01():\n",
    "    return (dt.date.today() - rtd.relativedelta(months=+1)  - rtd.relativedelta(day = 1)).strftime(\"%Y%m%d\")\n",
    "\n",
    "def f_t_now():\n",
    "    return dt.datetime.today().strftime(\"%H:%M:%S\")\n",
    "\n",
    "def f_d_now():\n",
    "    return dt.date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def f_dt_now():\n",
    "    return dt.datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Function simplicity. \n",
    "\n",
    "\n",
    "Same output, wastly different code (performance, and readability). Simplicity  matters! \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_apply_bin_reduction_method(exec_f, indata, list_reduction_method, sclr_pre_post_mode='pre'):\n",
    "\n",
    "    \"\"\"\n",
    "    This function itterates over a dataframe which has monotonically decreasing values, and bins the rows into larger groups based on a proportional reduction \n",
    "    coming from f_bin_reduction_method function. \n",
    "    \"\"\"\n",
    "\n",
    "    if exec_f:\n",
    "\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import time as time\n",
    "\n",
    "        df_temp=df_temp_pre.copy()\n",
    "        sclr_pre_post_mode='pre'\n",
    "\n",
    "        list_hld_df_idx=list()\n",
    "        list_hld_df=list()\n",
    "\n",
    "        print (\"Datetime is:  {}. Nr of rows going in is: {}\\n\".format(f_dt_now(),len(df_temp)))\n",
    "        tick=time.time()\n",
    "\n",
    "\n",
    "        #-------------------------------\n",
    "        # Prepp data before logic\n",
    "        #-------------------------------\n",
    "\n",
    "        # The values are increasing, i.e. we are monotonically increasing over distribution, pre mode point\n",
    "        if sclr_pre_post_mode=='pre':\n",
    "            print (\"Pre central mode point\\n\")\n",
    "\n",
    "            # Highest to lowest sort, and reset index!\n",
    "            df_temp=df_temp.sort_index(ascending=False)\n",
    "\n",
    "        # We are post mode point\n",
    "        else:\n",
    "            print (\"Post central mode point\\n\")\n",
    "\n",
    "\n",
    "        #-----------------------------------\n",
    "        # Itterate through reduction rules\n",
    "        #-----------------------------------\n",
    "        for idx_reduction_lvl, reduction_lvl in enumerate(list_reduction_method):\n",
    "\n",
    "            print (\"Processing reduction lvl: {} being a proportional shift of: {} from central mode point.\".format(idx_reduction_lvl, reduction_lvl))\n",
    "\n",
    "            #-------------------------------------------------------\n",
    "            # Reduction lvl 1: Whole DataFrame is processed\n",
    "            #-------------------------------------------------------\n",
    "            if idx_reduction_lvl==0:\n",
    "\n",
    "                # Temp variables (cum sum % and cum sum observations)\n",
    "                df_temp['tmp_cumsum_%']=np.cumsum(df_temp['count_obs_%_tot'])\n",
    "                df_temp['tmp_cumsum_obs']=np.cumsum(df_temp['count_obs'])\n",
    "\n",
    "                # what group are we processing?\n",
    "                df_temp['grp_reduction_lvl']=idx_reduction_lvl + 1\n",
    "\n",
    "                # Flag rows\n",
    "                df_temp['f_reduced_grp']=np.where(df_temp['tmp_cumsum_%']<=reduction_lvl\n",
    "                                                  ,1\n",
    "                                                  ,0\n",
    "                                                 )\n",
    "\n",
    "                # Append subset result from flag\n",
    "                list_hld_df.append(df_temp[df_temp['f_reduced_grp']==1])\n",
    "                list_hld_df_idx.extend(df_temp[df_temp['f_reduced_grp']==1].index.tolist())\n",
    "\n",
    "            #--------------------------------------------------------------------------------------------------\n",
    "            # Reduction lvl 1+i: Subset DataFrame is processed (removing previous processed index rows)\n",
    "            #--------------------------------------------------------------------------------------------------\n",
    "            else:\n",
    "\n",
    "                # Remove index rows we already have processed\n",
    "                df_temp_sub=df_temp[~df_temp.index.isin(list_hld_df_idx)]\n",
    "\n",
    "                # Temp variables (cum sum % and cum sum observations)\n",
    "                df_temp_sub['tmp_cumsum_%']=np.cumsum(df_temp_sub['count_obs_%_tot'])\n",
    "                df_temp_sub['tmp_cumsum_obs']=np.cumsum(df_temp_sub['count_obs'])\n",
    "\n",
    "                # what group are we processing?\n",
    "                df_temp_sub['grp_reduction_lvl']=idx_reduction_lvl + 1\n",
    "\n",
    "                # Flag rows\n",
    "                df_temp_sub['f_reduced_grp']=np.where(df_temp_sub['tmp_cumsum_%']<=reduction_lvl\n",
    "                                                  ,1\n",
    "                                                  ,0)\n",
    "\n",
    "                # Append subset result to holder\n",
    "                list_hld_df.append(df_temp_sub[df_temp_sub['f_reduced_grp']==1])\n",
    "                list_hld_df_idx.extend(df_temp_sub[df_temp_sub['f_reduced_grp']==1].index.tolist())\n",
    "\n",
    "        #-------------------------------------\n",
    "        # Append to a common DataFrame\n",
    "        #-------------------------------------\n",
    "        df_temp=pd.concat(list_hld_df, axis=0)\n",
    "\n",
    "        # Propagate bin edge, given pre or post mode point, throughout whole dataframe\n",
    "        if sclr_pre_post_mode=='pre':\n",
    "            df_temp['bin_edge_grp_lower'] = df_temp[['grp_reduction_lvl','bin_upper_edge']].groupby(['grp_reduction_lvl']).transform('first')\n",
    "\n",
    "        elif sclr_pre_post_mode == 'post':\n",
    "            df_temp['bin_edge_grp_lower'] = df_temp[['grp_reduction_lvl','bin_lower_edge']].groupby(['grp_reduction_lvl']).transform('first')\n",
    "\n",
    "            \n",
    "        print (\"\\nTotal elapsed time is: {}s. Nr of rows going out is: {}\".format(round(time.time()-tick), len(df_temp)))\n",
    "        \n",
    "        return df_temp\n",
    "\n",
    "    else:\n",
    "        print (\"No execution of function, ending...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_apply_bin_reduction_method(exec_f, indata, list_reduction_method, sclr_pre_post_mode='pre'):\n",
    "\n",
    "    \"\"\"\n",
    "    This function itterates over a dataframe which has monotonically decreasing values, and bins the rows into larger groups based on a proportional reduction \n",
    "    coming from f_bin_reduction_method function. \n",
    "    \"\"\"\n",
    "\n",
    "    # Modules\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import time as time\n",
    "\n",
    "    # Inputs\n",
    "    df_temp=indata.copy()\n",
    "    list_hld_reduction_method=list_reduction_method\n",
    "    \n",
    "    # time stamp\n",
    "    try:\n",
    "        print (\"Datetime now is: {}. Nr of rows going in is: {}\\n\".format(f_dt_now(), len(df_temp)))\n",
    "    \n",
    "    except:\n",
    "        print (\"No datetime function loaded\")\n",
    "    \n",
    "    # Start counting time\n",
    "    tick=time.time()\n",
    "\n",
    "    #-------------------------------\n",
    "    # Prepp data before logic\n",
    "    #-------------------------------\n",
    "           \n",
    "    # The values are increasing, i.e. we are monotonically increasing over distribution, pre mode point\n",
    "    if sclr_pre_post_mode=='pre':\n",
    "    \n",
    "        print (\"Pre central mode point\\n\")\n",
    "        bool_pre=True\n",
    "                            \n",
    "        # Highest to lowest sort, and make a copy so things stick! \n",
    "        df_temp=df_temp.sort_index(ascending=False)\n",
    "        df_temp_copy=df_temp.copy()\n",
    "        \n",
    "        # Reset index for itteration logic to work\n",
    "        df_temp.reset_index(drop=True, inplace=True)\n",
    "        df_temp_copy.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    # We are post mode point\n",
    "    else:\n",
    "        \n",
    "        print (\"post central mode point\\n\")\n",
    "        bool_pre=False\n",
    "        \n",
    "        # Make a copy so things stick! \n",
    "        df_temp_copy=df_temp.copy()\n",
    "        \n",
    "        # Reset index for itteration logic to work\n",
    "        df_temp.reset_index(drop=True, inplace=True)\n",
    "        df_temp_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        \n",
    "    #-------------------------------\n",
    "    # Apply itteration logic\n",
    "    #-------------------------------\n",
    "\n",
    "    \n",
    "    # Parameters and data holders\n",
    "    sclr_sum_prct_tot = 0\n",
    "    sclr_sum_count_tot = 0\n",
    "    sclr_reduction_lvl = 0\n",
    "    sclr_bin_edge=0\n",
    "    list_hld_df_data_reduced = list()\n",
    "    \n",
    "    # Itterate over data rows with mode point as reference. We write to the copy DataFrame to avoid things getting lost! \n",
    "    for df_row in df_temp.iterrows():\n",
    "\n",
    "        # Add together % of total, and count, given row and adherent bin\n",
    "        sclr_sum_prct_tot += df_row[1]['count_obs_%_tot']    \n",
    "        sclr_sum_count_tot += df_row[1]['count_obs']    \n",
    "\n",
    "        # Set group reduction lvl\n",
    "        df_temp_copy.loc[df_row[0], 'grp_reduction_lvl']=sclr_reduction_lvl\n",
    "\n",
    "        # Set group reduction lvl prct of total sum and count of total sum\n",
    "        df_temp_copy.loc[df_row[0], 'sum_prct_tot']=sclr_sum_prct_tot\n",
    "        df_temp_copy.loc[df_row[0], 'sum_count_tot']=sclr_sum_count_tot\n",
    "\n",
    "        # Start value: This picks up first row if we dont surpass the reduction level\n",
    "        if df_row[0]==0:\n",
    "            \n",
    "            # Initialize bin edges: We need to keep track of start and end points for each new bin, and also if the values are monotonic increase/decrease\n",
    "            if (bool_pre==True):\n",
    "                \n",
    "                # Start point: Set group reduction lvl count of total sum\n",
    "                sclr_bin_edge=df_temp_copy.loc[df_row[0], 'bin_upper_edge']\n",
    "                df_temp_copy.loc[df_row[0], 'bin_edge_lag1']=sclr_bin_edge\n",
    "\n",
    "            elif (bool_pre==False):\n",
    "\n",
    "                # Start point: Set group reduction lvl count of total sum\n",
    "                sclr_bin_edge=df_temp_copy.loc[df_row[0], 'bin_upper_edge']                \n",
    "                df_temp_copy.loc[df_row[0], 'bin_edge_lag1']=sclr_bin_edge\n",
    "            \n",
    "            # Push out start point, and lower/upper bin edge depending on monotonic increase/decrease\n",
    "            print (\"FIRST ROW: {} \\n\".format(df_row[0]))\n",
    "            list_hld_df_data_reduced.append(df_temp_copy[df_temp_copy.index==df_row[0]])\n",
    "\n",
    "            \n",
    "        # Populate bins edges\n",
    "        df_temp_copy.loc[df_row[0], 'bin_edge_lag1']=sclr_bin_edge\n",
    "\n",
    "    \n",
    "        # If we hit a given reduction level, mark the row, set counter to 0 and start with next reduction level. This will also overvrite first row above if we get a hit down here \n",
    "        if sclr_sum_prct_tot > list_hld_reduction_method[sclr_reduction_lvl]:\n",
    "\n",
    "            # Set group reduction lvl\n",
    "            df_temp_copy.loc[df_row[0], 'grp_reduction_lvl']=sclr_reduction_lvl\n",
    "\n",
    "            # Set group reduction lvl prct of total sum and count of total sum\n",
    "            df_temp_copy.loc[df_row[0], 'sum_prct_tot']=sclr_sum_prct_tot\n",
    "            df_temp_copy.loc[df_row[0], 'sum_count_tot']=sclr_sum_count_tot\n",
    "            \n",
    "            # Update bin edges: We need to keep track of start and end points, and also if the values are monotonic increasing or decreasing\n",
    "            if (bool_pre==True) & (df_row[0] > 0):\n",
    "                \n",
    "                # New update edge, looking back one row\n",
    "                sclr_bin_edge=df_temp_copy.loc[(df_row[0]-1), 'bin_upper_edge']\n",
    "                \n",
    "                # Set group reduction lvl count of total sum\n",
    "                df_temp_copy.loc[df_row[0], 'bin_edge_lag1']=sclr_bin_edge\n",
    "\n",
    "            # Update bin edges: We need to keep track of start and end points, and also if the values are monotonic increasing or decreasing\n",
    "            elif (bool_pre==False) & (df_row[0] > 0):\n",
    "                \n",
    "                # New update edge, looking back one row\n",
    "                sclr_bin_edge=df_temp_copy.loc[(df_row[0]-1), 'bin_lower_edge']\n",
    "                    \n",
    "                # Set group reduction lvl count of total sum\n",
    "                df_temp_copy.loc[df_row[0], 'bin_edge_lag1']=sclr_bin_edge\n",
    "                                \n",
    "            # Append the row to our reduced data frame\n",
    "            list_hld_df_data_reduced.append(df_temp_copy[df_temp_copy.index==df_row[0]])\n",
    "\n",
    "            # Increment reduction level counter and set sum prct holder variable and count variable to 0\n",
    "            sclr_reduction_lvl += 1\n",
    "            sclr_sum_prct_tot = 0\n",
    "            sclr_sum_count_tot = 0\n",
    "\n",
    "\n",
    "        # We hit the last row\n",
    "        if df_row[0]==len(df_temp)-1:\n",
    "\n",
    "            # Append the row to our reduced data frame\n",
    "            list_hld_df_data_reduced.append(df_temp_copy[df_temp_copy.index==df_row[0]])\n",
    "            \n",
    "            print (\"LAST ROW: {}\\n\".format(df_row[0]))\n",
    "\n",
    "    # Write back to original\n",
    "    df_temp=df_temp_copy.copy()\n",
    "\n",
    "    print (\"Total elasped time is: {}s. Nr of rows going out is: {}\".format(round(time.time()-tick, 2), len(df_temp)))\n",
    "    \n",
    "    return df_temp, list_hld_df_data_reduced\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
